# Papers from GTP4

**Foundations of Reinforcement Learning**

1. "Reinforcement Learning: An Introduction" - Richard S. Sutton and Andrew G. Barto (Book)
    - Not a paper, but this is an essential read to understand the fundamentals of RL.

**Exploration vs Exploitation**

2. "The Optimistic Principle: A Unified Framework for the Study of Reinforcement Learning" - Emilio Parisotto, Xingyou Song, Vincent Dumoulin, Negar Rostamzadeh, Christopher J. Pal, Yoshua Bengio, and Ruslan Salakhutdinov.
3. "Asymptotically Optimal Algorithm for Finite-Armed and Continuous-Armed Bandits" - Tor Lattimore, Csaba Szepesvári.

**Value Iteration, Policy Iteration, and Q-Learning**

4. "Convergence of Q-learning: A Simple Proof" - Francisco S. Melo.
5. "Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets" - Marc' Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba.

**Policy Gradient Methods**

6. "Policy Gradient Methods for Reinforcement Learning with Function Approximation" - Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour.
7. "High-Dimensional Continuous Control Using Generalized Advantage Estimation" - John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.

**Deep Q-Learning**

8. "Playing Atari with Deep Reinforcement Learning" - Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller.
9. "Deep Reinforcement Learning with Double Q-Learning" - Hado van Hasselt, Arthur Guez, and David Silver.

**Actor-Critic Methods**

10. "Deterministic Policy Gradient Algorithms" - David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
11. "Continuous control with deep reinforcement learning" - Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra.

**Model-Based Reinforcement Learning**

12. "Dyna, an Integrated Architecture for Learning, Planning, and Reacting" - Richard S. Sutton.
13. "Imagination-Augmented Agents for Deep Reinforcement Learning" - Théophane Weber, Sébastien Racanière, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Rezende, Adria Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, Daan Wierstra.

**Inverse Reinforcement Learning**

14. "Algorithms for Inverse Reinforcement Learning" - Andrew Y. Ng and Stuart Russell.
15. "Generative Adversarial Imitation Learning" - Jonathan Ho and Stefano Ermon.

**Multi-Agent Reinforcement Learning**

16. "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations" - Yoav Shoham and Kevin Leyton-Brown (Book)
17. "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning" - Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel.

# Open AI Papers (GPT4)

1. "Proximal Policy Optimization Algorithms" - John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
    - Introduces the Proximal Policy Optimization (PPO) algorithm, a widely used method in RL.

2. "Distributed Prioritized Experience Replay" - Tom Schaul, John Quan, Ioannis Antonoglou, David Silver.
    - Presents an efficient, distributed variant of prioritized experience replay, a critical technique for experience management in RL.

3. "Reinforcement Learning with Augmented Data" - Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas.
    - Explains how to use data augmentation to significantly improve the sample efficiency of RL algorithms.

4. "Learning Dexterity" - OpenAI team.
    - Describes the use of RL to train a robotic hand to solve a Rubik's Cube, showcasing RL's potential for complex, real-world tasks.

5. "DALL-E: Creating Images from Text" - Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever.
    - Not strictly an RL paper, but it shows how RL and other techniques can be combined to solve complex AI tasks.

6. "GPT-3: Language Models are Few-Shot Learners" - Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei.
    - A very influential paper that, while primarily about language modeling, incorporates concepts from RL, showcasing the interplay between different areas of AI research.

# Google Transformers Papers

1. "Attention is All You Need" - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.
    - This is the seminal paper introducing the Transformer model, which has revolutionized many areas of NLP.

2. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" - Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova.
    - Presents the BERT model, a bidirectional Transformer that has achieved state-of-the-art results on a variety of NLP tasks.

3. "The Illustrated Transformer" - Jay Alammar.
    - Although not a research paper, this resource provides excellent visual explanations of Transformer models and is very helpful for understanding the above papers.

4. "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" - Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.
    - Introduces the T5 model, a flexible approach that casts all NLP tasks into a text-to-text format.

5. "XLNet: Generalized Autoregressive Pretraining for Language Understanding" - Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.
    - Proposes the XLNet, which overcomes some limitations of BERT by using a permutation-based training strategy.

6. "Big Bird: Transformers for Longer Sequences" - Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.
    - Presents the BigBird model, which extends the standard Transformer to efficiently handle longer sequences of text.
