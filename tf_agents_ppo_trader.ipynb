{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS21XA7BzVZXXnyQInCzJK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j03m/lstm-price-predictor/blob/main/tf_agents_ppo_trader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_agents\n",
        "!pip install yfinance\n",
        "!pip install tensorflow-probability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO3iaffdNy31",
        "outputId": "b6ef3841-b751-4694-b85d-fce60272beca"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf_agents in /usr/local/lib/python3.9/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (1.22.4)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (1.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (1.15.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from tf_agents) (8.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (4.5.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (2.2.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (1.15.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (0.23.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (0.19.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (3.19.6)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (2.1.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (0.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.9/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (6.0.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability>=0.18.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability>=0.18.0->tf_agents) (0.1.8)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability>=0.18.0->tf_agents) (0.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf_agents) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.9/dist-packages (0.2.12)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.3.5)\n",
            "Requirement already satisfied: cryptography>=3.3.2 in /usr/local/lib/python3.9/dist-packages (from yfinance) (39.0.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.9/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.11.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2022.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0,'/content/drive/My Drive/ml-trde-notebooks')\n",
        "data_path = '/content/drive/My Drive/ml-trde-notebooks/data'\n",
        "model_path = \"/content/drive/My Drive/ml-trde-notebooks/models\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZpUrmWiOb8l",
        "outputId": "df529744-e76a-46b6-b549-5a4baaa623d4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tf_agents\n",
        "from tf_agents.environments import suite_gym, tf_py_environment\n",
        "from tf_agents.networks import network, utils, categorical_projection_network\n",
        "from tf_agents.agents.ppo import ppo_agent\n",
        "from tf_agents.policies import actor_policy\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.networks import actor_distribution_network, value_network\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.environments import parallel_py_environment\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from scipy.stats import norm\n",
        "\n",
        "import multiprocessing\n",
        "from itertools import cycle\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "print(\"I has cores: \", cores)\n",
        "try:\n",
        "  tf_agents.system.multiprocessing.enable_interactive_mode()\n",
        "except Exception as inst:\n",
        "  pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD52woTyQUsM",
        "outputId": "7068ee6a-426c-424b-f535-60beb8dd2911"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I has cores:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Executes this notebook in our space, making all of its functions/globals available\n",
        "%run -i '/content/drive/My Drive/ml-trde-notebooks/backtest-common.ipynb'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrk7MuAYOXqd",
        "outputId": "70dbcd01-6674-4989-b210-b180d71a5c09"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified\n",
            "1679185329\n",
            "1679185329.0\n",
            "1679185329.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging_level = 2\n",
        "def error(*args):\n",
        "    if logging_level >= 0:\n",
        "        print(*args)\n",
        "\n",
        "def info(*args):\n",
        "    if logging_level >=1:\n",
        "        print(*args)\n",
        "\n",
        "def verbose(*args):\n",
        "    if logging_level >=2:\n",
        "        print(*args)\n",
        "\n",
        "def debug(*args):\n",
        "    if logging_level >=3:\n",
        "        print(*args)"
      ],
      "metadata": {
        "id": "yhAiVGS6_bZG"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_stocks():\n",
        "  # We don't want to train against crypto\n",
        "  return get_all_stock_timerseries_for_csv(\"training_tickers3.csv\", 3500, 10)\n",
        "\n",
        "LENGTH_OF_STOCK_TRAINGING_DATA = 145 #I might need to fix this, but the model is tied to the number of symbols we trained on\n",
        "def download_crypto():\n",
        "  return get_all_product_timeseries(-1, 180, LENGTH_OF_STOCK_TRAINGING_DATA)"
      ],
      "metadata": {
        "id": "X1VtA9cSRAEe"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from_disk = False\n",
        "use_crypto = False\n",
        "\n",
        "if from_disk:\n",
        "  product_data = load_dict(use_crypto)\n",
        "  first_key = list(product_data.keys())[0]\n",
        "  length = len(product_data[first_key])\n",
        "elif use_crypto:\n",
        "  product_data = download_crypto()\n",
        "  save_dict(product_data, True)\n",
        "else:\n",
        "  product_data = download_stocks()\n",
        "  save_dict(product_data)\n",
        "\n",
        "min_len = min(len(df) for df in product_data.values())\n",
        "print(\"symbols:\", len(product_data.keys()))\n",
        "print(\"min length:\", min_len)\n",
        "for name, df in product_data.items():\n",
        "    product_data[name] = df.head(min_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neo9ZYMWQXUs",
        "outputId": "a4a8869a-f265-46de-8adf-2730e4f20ab0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading ticker:  XTSLA  count: 0 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "\n",
            "1 Failed download:\n",
            "- XTSLA: No timezone found, symbol may be delisted\n",
            "loading ticker:  XEL  count: 1 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "loading ticker:  WU  count: 2 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "loading ticker:  WTW  count: 3 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "loading ticker:  WST  count: 4 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "loading ticker:  WMT  count: 5 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "loading ticker:  WMB  count: 6 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "loading ticker:  WM  count: 7 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "loading ticker:  WEC  count: 8 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "loading ticker:  WCN  count: 9 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "loading ticker:  VZ  count: 10 of: 164\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "loading ticker:  VRTX  count: 11 of: 164\n",
            "symbols: 10\n",
            "min length: 3500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gym import spaces\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "class TraderEnv(py_environment.PyEnvironment):\n",
        "    \n",
        "    def __init__(self, product, df):\n",
        "        super(TraderEnv, self).__init__()\n",
        "\n",
        "        # Define observation spec\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(4,), dtype=np.float32, minimum=0, maximum=np.inf, name='observation'\n",
        "        )\n",
        "\n",
        "        # Define action spec\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=1, name='action'\n",
        "        )\n",
        "\n",
        "        # Initialize environment state\n",
        "        df = self.expand(df.copy())\n",
        "      \n",
        "        self.timeseries = self.scale(df[[\"Date\", \"Close\", \"weighted-volume\", \"trend\", \"prob_above_trend\"]])\n",
        "        self.product = product\n",
        "        self.final = len(df)\n",
        "        self._reset_vars()\n",
        "    \n",
        "    def expand(self, df):\n",
        "      # Perform seasonal decomposition\n",
        "      result = seasonal_decompose(df['Close'], model='additive', period=90, extrapolate_trend='freq')\n",
        "\n",
        "      # Add trend back to original time series\n",
        "      df[\"trend\"] = result.trend\n",
        "\n",
        "      # Compute the residuals by subtracting the trend from the original time series\n",
        "      residuals = result.resid\n",
        "\n",
        "      # Fit a Gaussian distribution to the residuals\n",
        "      mu, std = norm.fit(residuals)\n",
        "\n",
        "      # Compute the probability of a value being above or below the trend line\n",
        "      # for each point in the time series\n",
        "      z_scores = residuals / std\n",
        "      df[\"prob_above_trend\"] = 1 - norm.cdf(z_scores)\n",
        "      df[\"weighted-volume\"] = df[\"Close\"] * df[\"Volume\"]\n",
        "      return df\n",
        "\n",
        "    def scale(self, timeseries):\n",
        "      df = timeseries.reset_index()  # Reset the index of the DataFrame\n",
        "      dates = df['Date']\n",
        "      data_to_scale = df.drop('Date', axis=1)\n",
        "      self.scaler = MinMaxScaler()\n",
        "      scaled_data = self.scaler.fit_transform(data_to_scale)\n",
        "      scaled_df = pd.concat([dates, pd.DataFrame(scaled_data, columns=data_to_scale.columns)], axis=1)\n",
        "      return scaled_df.set_index('Date')  # Set the index back to 'Date'\n",
        "\n",
        "    def make_ledger_row(self):\n",
        "        ledger = pd.DataFrame()\n",
        "        ledger[\"Date\"] = []\n",
        "        ledger[\"Side\"] = []\n",
        "        ledger[\"Action\"]  = []\n",
        "        ledger[\"Price\"] = []\n",
        "        ledger[\"Fee\"] = []\n",
        "        return ledger\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset_vars(self):\n",
        "        self._state = None\n",
        "        self._episode_ended = False        \n",
        "        self.ledger = self.make_ledger_row()\n",
        "        self.slippage = .01\n",
        "        self.fee = .0025\n",
        "        self.current_index = 0\n",
        "        self.cash = 5000\n",
        "        self.initial_cash = self.cash\n",
        "        self.position_shares = 0\n",
        "        self.position_value = 0\n",
        "        self._state = self._get_initial_state()\n",
        "\n",
        "    def _reset(self):\n",
        "        # Reset the environment and return the initial time step\n",
        "        self._reset_vars()\n",
        "        return ts.restart(self._state)\n",
        "\n",
        "    def _step(self, action):\n",
        "        if self._episode_ended:\n",
        "            # The last action ended the episode. Ignore the current action and start a new episode.\n",
        "            return self.reset()\n",
        "\n",
        "        # Apply the action and update the environment state\n",
        "        self._apply_action(action)\n",
        "        self._state = self._get_next_state()\n",
        "\n",
        "        if self._is_episode_ended():\n",
        "            self._episode_ended = True\n",
        "            reward = self._get_final_reward()\n",
        "            return ts.termination(self._state, reward)\n",
        "        else:\n",
        "            reward = self._get_reward()\n",
        "            return ts.transition(self._state, reward=reward, discount=1.0)\n",
        "\n",
        "    def _get_initial_state(self):\n",
        "        # Return the initial state of the environment\n",
        "        self.current_index = 0\n",
        "        return self.env_block()\n",
        "\n",
        "    def _apply_action(self, action):\n",
        "        # Advance the environment by one time step and return the observation, reward, and done flag\n",
        "        self.current_index += 1\n",
        "        info(\"step:\", \"index:\", self.current_index, \" of: \", self.final-1, \" cash: \", self.cash, \" value: \", self.position_value)\n",
        "\n",
        "        if (self.current_index >= self.final - 1 or  self._get_reward() <= -1):\n",
        "          info(\"********MARKING DONE\", \"index:\", self.current_index, \" of: \", self.final-1, \" cash: \", self.cash, \" value: \", self.position_value)\n",
        "          if (self.position_shares != 0):\n",
        "            info(\"done so closing position\")\n",
        "            self.close_position()\n",
        "          self._episode_ended = True\n",
        "        else:\n",
        "          self._episode_ended = False\n",
        "\n",
        "    \n",
        "        if (action == 1 and self.position_shares != 0):\n",
        "          info(\"holding: \")\n",
        "          self.update_position_value()\n",
        "        elif (action == 0):\n",
        "          info(\"closing: \")\n",
        "          self.close_position()\n",
        "        elif (action == 1 and self.position_shares == 0):\n",
        "          info(\"opening: \")\n",
        "          self.open_position()\n",
        "        else: #assume hold\n",
        "          self.update_position_value()\n",
        "\n",
        "    def _get_next_state(self):\n",
        "        # Calculate and return the next state based on the current state and action taken\n",
        "        return self.env_block()\n",
        "\n",
        "    def update_position_value(self):\n",
        "        df = self.timeseries\n",
        "        row = df.iloc[self.current_index,:]\n",
        "        self.position_value = row[\"Close\"] * self.position_shares\n",
        "\n",
        "    def _is_episode_ended(self):\n",
        "        return self.current_index >= self.final\n",
        "        \n",
        "\n",
        "    def _get_reward(self):\n",
        "        self.update_position_value()\n",
        "        current_portfolio_value = self.position_value + self.cash\n",
        "        percentage_change = (current_portfolio_value - self.initial_cash) / self.initial_cash\n",
        "        clipped_reward = np.clip(percentage_change, -1, np.inf)\n",
        "        return clipped_reward\n",
        "\n",
        "    def _get_final_reward(self):\n",
        "        # Calculate and return the final reward when the episode ends\n",
        "        return self._get_reward()\n",
        "\n",
        "    def env_block(self):\n",
        "      start_index = self.current_index\n",
        "      end_index = self.current_index\n",
        "      df = self.timeseries.copy()\n",
        "      df = self.timeseries.reset_index().drop(['Date'], axis=1)\n",
        "      block = df.iloc[start_index].to_numpy()\n",
        "      return block\n"
      ],
      "metadata": {
        "id": "m4bShVXE1rZv"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "kAwC_JEZn__c",
        "outputId": "8c9bf1b2-bfe3-47fd-a7d0-ab15383824c3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-6e721f96677f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mclip_pg_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m tf_agent = tf_agents.agents.PPOAgent(\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m       \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/agents/ppo/ppo_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, time_step_spec, action_spec, optimizer, actor_net, value_net, greedy_eval, importance_ratio_clipping, lambda_value, discount_factor, entropy_regularization, policy_l2_reg, value_function_l2_reg, shared_vars_l2_reg, value_pred_loss_coef, num_epochs, use_gae, use_td_lambda_return, normalize_rewards, reward_norm_clipping, normalize_observations, log_prob_clipping, kl_cutoff_factor, kl_cutoff_coef, initial_adaptive_kl_beta, adaptive_kl_target, adaptive_kl_tolerance, gradient_clipping, value_clipping, check_numerics, compute_value_and_advantage_in_train, update_normalizers_in_train, aggregate_losses_across_replicas, debug_summaries, summarize_grads_and_vars, train_step_counter, name)\u001b[0m\n\u001b[1;32m    374\u001b[0m       \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGreedyPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     collect_policy = ppo_policy.PPOPolicy(\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0mtime_step_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0maction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m       \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/agents/ppo/ppo_policy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, time_step_spec, action_spec, actor_network, value_network, observation_normalizer, clip, collect, compute_value_and_advantage_in_train)\u001b[0m\n\u001b[1;32m    140\u001b[0m         info_spec = {\n\u001b[1;32m    141\u001b[0m             \u001b[0;34m'dist_params'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 tf.nest.map_structure(nested_dist_params,\n\u001b[0m\u001b[1;32m    143\u001b[0m                                       actor_output_spec)\n\u001b[1;32m    144\u001b[0m         }\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/agents/ppo/ppo_policy.py\u001b[0m in \u001b[0;36mnested_dist_params\u001b[0;34m(spec)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mnested_dist_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionSpecV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0;34m'Unexpected output from `actor_network`.  Expected '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;34m'`Distribution` objects, but saw output spec: {}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected output from `actor_network`.  Expected `Distribution` objects, but saw output spec: TensorSpec(shape=(), dtype=tf.int32, name=None)\n  In call to configurable 'PPOPolicy' (<class 'tf_agents.agents.ppo.ppo_policy.PPOPolicy'>)\n  In call to configurable 'PPOAgent' (<class 'tf_agents.agents.ppo.ppo_agent.PPOAgent'>)"
          ]
        }
      ],
      "source": [
        "env_count = 0\n",
        "num_parallel_envs = 4\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 25\n",
        "discount_factor = 0.99\n",
        "gradient_clipping = 0.5\n",
        "entropy_regularization = 1e-2\n",
        "value_pred_loss_coef = 0.5\n",
        "use_gae = True\n",
        "use_td_lambda_return = True\n",
        "actor_loss_weight = 1.0\n",
        "value_loss_weight = 0.5\n",
        "log_interval = 50\n",
        "eval_interval = 100\n",
        "\n",
        "def create_env():\n",
        "    global env_count\n",
        "    products = list(product_data.keys())\n",
        "    product = products[env_count]\n",
        "    env = TraderEnv(product, product_data[product])\n",
        "    env_count+=1\n",
        "    return env\n",
        "\n",
        "def collect_episode(environment, policy):\n",
        "    episode = []\n",
        "    time_step = environment.reset()\n",
        "    while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        next_time_step = environment.step(action_step.action)\n",
        "        episode.append((time_step, action_step, next_time_step.reward))\n",
        "        time_step = next_time_step\n",
        "    return episode\n",
        "\n",
        "def train_one_episode(agent, environment):\n",
        "    episode = collect_episode(environment, agent.collect_policy)\n",
        "    experience = to_trajectories(episode)\n",
        "    train_loss = agent.train(experience=experience)\n",
        "    return train_loss, sum([step[2] for step in episode])\n",
        "\n",
        "def to_trajectories(episode):\n",
        "    time_steps, actions, rewards = zip(*episode)\n",
        "    return trajectory.Trajectory(\n",
        "        observation=tf.nest.map_structure(np.stack, [t.observation for t in time_steps]),\n",
        "        action=tf.nest.map_structure(np.stack, [a.action for a in actions]),\n",
        "        policy_info=(),\n",
        "        reward=tf.stack(rewards),\n",
        "        discount=tf.constant([1.0] * len(rewards), dtype=tf.float32),\n",
        "        step_type=tf.nest.map_structure(np.stack, [t.step_type for t in time_steps]),\n",
        "        next_step_type=tf.nest.map_structure(np.stack, [t.step_type for t in time_steps[1:]] + [time_steps[-1].step_type]),\n",
        "    )\n",
        "\n",
        "def lstm_layers(input, lstm_units=64, dropout_rate=0.3):\n",
        "    \n",
        "    batch_size = input.shape[0]  # Get the batch size from the input tensor\n",
        "    features = input.shape[1]   # Get the number of features from the input tensor\n",
        "    timestep = 1  # Set the time step dimension size\n",
        "\n",
        "    # Reshape the input tensor to have an additional time step dimension\n",
        "    x = tf.keras.layers.Reshape((batch_size, features))(input)\n",
        "\n",
        "    x = tf.keras.layers.LSTM(lstm_units, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LSTM(lstm_units, return_sequences=True)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LSTM(lstm_units)(x)\n",
        "    \n",
        "    # Add the final dense layer with sigmoid activation\n",
        "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_episodes):\n",
        "        time_step = environment.reset()\n",
        "        episode_return = 0.0\n",
        "        while not time_step.is_last():\n",
        "            action_step = policy.action(time_step)\n",
        "            time_step = environment.step(action_step.action)\n",
        "            episode_return += time_step.reward\n",
        "        total_return += episode_return\n",
        "\n",
        "    avg_return = total_return / num_episodes\n",
        "    return avg_return.numpy()[0]\n",
        "\n",
        "class ActorNetworkWithLSTM(network.Network):\n",
        "\n",
        "    def __init__(self, observation_spec, action_spec, lstm_units=64):\n",
        "        super(ActorNetworkWithLSTM, self).__init__(\n",
        "            input_tensor_spec=observation_spec,\n",
        "            state_spec=(),\n",
        "            name='ActorNetworkWithLSTM'\n",
        "        )\n",
        "        self._lstm_units = lstm_units\n",
        "        self._input_layer = tf.keras.layers.InputLayer(input_shape=observation_spec.shape)\n",
        "        self._lstm_layers = lstm_layers\n",
        "        \n",
        "        \n",
        "\n",
        "    def call(self, observations, step_type=None, network_state=()):\n",
        "        x = self._input_layer(observations)\n",
        "        logits = self._lstm_layers(x, lstm_units=self._lstm_units)\n",
        "        discrete_actions = tf.cast(output > 0.5, dtype=tf.int32)\n",
        "        return tf.squeeze(discrete_actions, axis=-1), network_state\n",
        "        return logits, network_state\n",
        "\n",
        "\n",
        "class ValueNetworkWithLSTM(network.Network):\n",
        "\n",
        "    def __init__(self, observation_spec, lstm_units=64):\n",
        "        super(ValueNetworkWithLSTM, self).__init__(\n",
        "            input_tensor_spec=observation_spec,\n",
        "            state_spec=(),\n",
        "            name='ValueNetworkWithLSTM'\n",
        "        )\n",
        "\n",
        "        self._lstm_units = lstm_units\n",
        "        self._input_layer = tf.keras.layers.InputLayer(input_shape=observation_spec.shape)\n",
        "        self._lstm_layers = lstm_layers\n",
        "        self._dense_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, observations, step_type=None, network_state=()):\n",
        "        x = self._input_layer(observations)\n",
        "        x = self._lstm_layers(x, lstm_units=self._lstm_units)\n",
        "        value = self._dense_layer(x)\n",
        "        return tf.reshape(value, [-1]), network_state\n",
        "\n",
        "\n",
        "# Create the environment\n",
        "all_envs = [create_env() for _ in range(len(product_data))]\n",
        "\n",
        "train_parallel_py_envs = [lambda: env for env in all_envs]\n",
        "train_parallel_env = parallel_py_environment.ParallelPyEnvironment(train_parallel_py_envs)\n",
        "\n",
        "train_env = tf_agents.environments.tf_py_environment.TFPyEnvironment(train_parallel_env)\n",
        "\n",
        "# Create the actor and value networks\n",
        "actor_net = ActorNetworkWithLSTM(train_env.observation_spec(), train_env.action_spec())\n",
        "value_net = ValueNetworkWithLSTM(train_env.observation_spec())\n",
        "\n",
        "# Define optimizer, PPO hyperparameters, and create the PPO agent\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "clip_rho_threshold = 0.2\n",
        "clip_pg_threshold = 0.2\n",
        "\n",
        "tf_agent = tf_agents.agents.PPOAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    optimizer,\n",
        "    actor_net=actor_net,\n",
        "    value_net=value_net,\n",
        "    num_epochs=num_epochs,\n",
        "    discount_factor=discount_factor,\n",
        "    importance_ratio_clipping=clip_pg_threshold,\n",
        "    entropy_regularization=entropy_regularization,\n",
        "    gradient_clipping=gradient_clipping,\n",
        "    value_pred_loss_coef=value_pred_loss_coef,\n",
        "    use_gae=True,\n",
        ")\n",
        "\n",
        "# Initialize the policy\n",
        "tf_agent.initialize()\n",
        "\n",
        "# Main training loop\n",
        "num_iterations = 1000\n",
        "collect_episodes_per_iteration = 50\n",
        "\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    # Collect a few episodes using collect_policy and save to the replay buffer.\n",
        "    total_loss = 0\n",
        "    for _ in range(collect_episodes_per_iteration):\n",
        "        train_loss = train_one_episode(tf_agent, train_env)\n",
        "        total_loss += train_loss.loss.numpy()\n",
        "    \n",
        "    avg_loss = total_loss / collect_episodes_per_iteration\n",
        "\n",
        "    step = tf_agent.train_step_counter.numpy()\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "        print(f'step = {step}: loss = {avg_loss}')\n",
        "\n",
        "\n",
        "\n",
        "policy = tf_agent.policy\n",
        "tf.saved_model.save(policy, model_path + \"/ppo\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict\n",
        "# Load the saved policy\n",
        "load = False\n",
        "if load:\n",
        "  loaded_policy = tf.saved_model.load(model_path + \"/ppo\")\n",
        "def predict(policy_path, env, loaded_policy, num_steps=100):\n",
        "    \n",
        "    # Reset the environment to get the initial observation\n",
        "    time_step = env.reset()\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        # Use the loaded policy to choose an action based on the observation\n",
        "        action_step = loaded_policy.action(time_step)\n",
        "\n",
        "        # Execute the chosen action in the environment\n",
        "        next_time_step = env.step(action_step.action)\n",
        "\n",
        "        # Extract the observation, reward, and other information\n",
        "        observation = next_time_step.observation\n",
        "        reward = next_time_step.reward\n",
        "        done = next_time_step.is_last()\n",
        "\n",
        "        print(f\"Step {step + 1}:\")\n",
        "        print(f\"Observation: {observation}\")\n",
        "        print(f\"Action: {action_step.action}\")\n",
        "        print(f\"Reward: {reward}\")\n",
        "        print(f\"Done: {done}\")\n",
        "        print()\n",
        "\n",
        "        # Update the time_step for the next iteration\n",
        "        time_step = next_time_step\n",
        "\n",
        "        # Break the loop if the episode is finished\n",
        "        if done:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "YMKr3pFr-KHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ERDs3wx-F-",
        "outputId": "1ea6dd37-be24-4702-de56-ae7a33c7322c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.9/dist-packages (0.19.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability) (0.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability) (1.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability) (0.1.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability) (4.4.2)\n"
          ]
        }
      ]
    }
  ]
}