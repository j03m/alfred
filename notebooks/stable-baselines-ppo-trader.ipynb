{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MO3iaffdNy31"},"outputs":[],"source":["!pip install git+https://github.com/DLR-RM/stable-baselines3@feat/gymnasium-support\n","!pip install git+https://github.com/Stable-Baselines-Team/stable-baselines3-contrib@feat/gymnasium-support\n","!pip install yfinance\n","!pip install gymnasium\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mZ4NgyLielD"},"outputs":[],"source":["!pip install sb3-contrib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25824,"status":"ok","timestamp":1682684636460,"user":{"displayName":"Joseph Mordetsky","userId":"12205681246264358771"},"user_tz":240},"id":"EZpUrmWiOb8l","outputId":"65d088d2-eff9-4137-c1be-c0087ddbcad0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","import os\n","\n","sys.path.insert(0,'/content/drive/My Drive/ml-trde-notebooks')\n","data_path = '/content/drive/My Drive/ml-trde-notebooks/data'\n","model_path = \"/content/drive/My Drive/ml-trde-notebooks/models\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8626,"status":"ok","timestamp":1682684645082,"user":{"displayName":"Joseph Mordetsky","userId":"12205681246264358771"},"user_tz":240},"id":"YD52woTyQUsM","outputId":"4f837dbe-4c95-417e-daba-62648ede1870"},"outputs":[{"output_type":"stream","name":"stdout","text":["I has cores:  12\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import math\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from scipy.stats import norm\n","\n","import multiprocessing\n","\n","from stable_baselines3 import PPO\n","from sb3_contrib import RecurrentPPO\n","from stable_baselines3.ppo.policies import MlpPolicy\n","\n","from stable_baselines3.common.evaluation import evaluate_policy\n","\n","from sb3_contrib.ppo_recurrent.policies import MlpLstmPolicy\n","from typing import NamedTuple, Tuple\n","import torch as th\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import os\n","import sys\n","import datetime\n","\n","import numpy as np\n","import gymnasium as gym\n","from gymnasium import logger, spaces\n","\n","import warnings\n","# filter out UserWarning messages\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","cores = multiprocessing.cpu_count()\n","print(\"I has cores: \", cores)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1015,"status":"ok","timestamp":1682684646094,"user":{"displayName":"Joseph Mordetsky","userId":"12205681246264358771"},"user_tz":240},"id":"zrk7MuAYOXqd","outputId":"9e0ffa07-4e2e-4f64-dd2a-d715c685885c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Modified\n","1679922395\n","1679922395.0\n","1679922395.0\n"]}],"source":["# Executes this notebook in our space, making all of its functions/globals available\n","%run -i '/content/drive/My Drive/ml-trde-notebooks/backtest-common.ipynb'"]},{"cell_type":"markdown","metadata":{"id":"CF9oPeaEEh2x"},"source":["# Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhAiVGS6_bZG"},"outputs":[],"source":["logging_level = 1\n","output_file = open(os.path.join(data_path, 'ppo-env.txt'), 'w')\n","def to_file(*args):\n","  output_str = ' '.join(str(arg) for arg in args)\n","  output_file.write(output_str)\n","  output_file.flush()\n","  \n","def error(*args):\n","    if logging_level >= 0:\n","        to_file(*args)\n","        print(*args)\n","\n","def info(*args):\n","    if logging_level >=1:\n","        to_file(*args)\n","        print(*args)\n","\n","def verbose(*args):\n","    if logging_level >=2:\n","        to_file(*args)\n","        print(*args)\n","\n","def debug(*args):\n","    if logging_level >=3:\n","        to_file(*args)\n","        print(*args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1VtA9cSRAEe"},"outputs":[],"source":["def download_stocks(total):\n","  # We don't want to train against crypto\n","  return get_all_stock_timerseries_for_csv(\"training_tickers3.csv\", 3500, total)\n","\n","LENGTH_OF_STOCK_TRAINGING_DATA = 145 #I might need to fix this, but the model is tied to the number of symbols we trained on\n","def download_crypto():\n","  return get_all_product_timeseries(-1, 180, LENGTH_OF_STOCK_TRAINGING_DATA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neo9ZYMWQXUs"},"outputs":[],"source":["def get_data_for_training(num):\n","\n","  from_disk = False\n","  use_crypto = False\n","\n","  if from_disk:\n","    product_data = load_dict(use_crypto)\n","    first_key = list(product_data.keys())[0]\n","    length = len(product_data[first_key])\n","  elif use_crypto:\n","    product_data = download_crypto()\n","    save_dict(product_data, True)\n","  else:\n","    product_data = download_stocks(num)\n","    save_dict(product_data)\n","\n","  min_len = min(len(df) for df in product_data.values())\n","  print(\"symbols:\", len(product_data.keys()))\n","  print(\"min length:\", min_len)\n","  for name, df in product_data.items():\n","      product_data[name] = df.head(min_len)\n","\n","  product_data = dict(sorted(product_data.items(), reverse=True))\n","\n","  return product_data"]},{"cell_type":"markdown","metadata":{"id":"y4ozSzUkElgr"},"source":["# Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4bShVXE1rZv"},"outputs":[],"source":["class TraderEnv(gym.Env):\n","    \n","    def __init__(self, product, df, curriculum_code=1):\n","        super(TraderEnv, self).__init__()\n","\n","        # Define the bounds for each column\n","        close_min, close_max = 0, np.inf  \n","        volume_min, volume_max = 0, np.inf  \n","        trend_min, trend_max = 0, np.inf  \n","        percentile_min, percentile_max = 0, 1\n","\n","        # Define the observation space\n","        self.observation_space = spaces.Box(low=np.array([close_min, volume_min, trend_min, percentile_min]),\n","                                      high=np.array([close_max, volume_max, trend_max, percentile_max]),\n","                                      dtype=np.float32)\n","\n","        # We have 3 actions, hold (0), long (1), short (2)\n","        self.action_space = spaces.Discrete(3)\n","        \n","        # Initialize environment state\n","        df = self.expand(df.copy())\n","\n","        self.orig_timeseries = df\n","        self.timeseries = self.scale(df[[\"Date\", \"Close\", \"weighted-volume\", \"trend\", \"prob_above_trend\"]])\n","        \n","        self._reset_vars()\n","        self.calculate_benchmark_metrics()\n","\n","\n","        # Don't add this here. Process the tickers into data files - you already have some.\n","        #self.lstm_internal = keras.models.load_model(model_path + \"/lstm_cv-4.h15\")\n","        #predictions = self.predict_trade(self.lstm_internal, self.timeseries[[\"Close\", \"weighted-volume\"]])\n","        #self.timeseries[\"predicted-price\"] = predictions\n","        \n","        self.product = product\n","        self.final = len(df)\n","\n","        self.curriculum_code = curriculum_code\n","        self.rolling_score = 0\n","        self.prob_high = 0.8\n","        self.prob_low = 0.2\n","        self.expert_actions = []\n","    \n","    def expert_opinion(self):\n","      df = self.timeseries\n","\n","      def assign_action(prob):\n","          if prob >= self.prob_high:\n","              return 1\n","          elif prob <= self.prob_low:\n","              return 2\n","          else:\n","              return 0\n","\n","      state_action_data = []\n","      \n","      for index, row in df.iterrows():\n","          state = row.values  # Keep all columns, including 'prob_above_trend', as part of the state\n","          action = assign_action(row[\"prob_above_trend\"])\n","          state_action_data.append((state, action))\n","          self.expert_actions.append(action)\n","      return state_action_data\n","\n","    def expert_opinion_df(self):\n","      df = self.timeseries\n","      \n","      # iterate over the time series\n","      def assign_action(prob):\n","          if prob >= self.prob_high:\n","              return 1\n","          elif prob <= self.prob_low:\n","              return 2\n","          else:\n","              return 0\n","\n","      # Create the new column 'action' based on the values in 'prob_above_trend'\n","      df['action'] = df['prob_above_trend'].apply(assign_action)\n","      self.expert_actions = df['action'].values\n","\n","    def calculate_benchmark_metrics(self):\n","      df = self.orig_timeseries\n","      row = df.iloc[0,:]\n","      price = self.get_price_with_slippage(row[\"Close\"])\n","      self.benchmark_position_shares = math.floor(self.cash/price)\n","     \n","    def predict_trade(self, model, X):\n","      predicted = model.predict(X, verbose=0).flatten()\n","      return predicted\n","\n","    def expand(self, df):\n","      # Perform seasonal decomposition\n","      result = seasonal_decompose(df['Close'], model='additive', period=90, extrapolate_trend='freq')\n","\n","      # Add trend back to original time series\n","      df[\"trend\"] = result.trend\n","\n","      # Compute the residuals by subtracting the trend from the original time series\n","      residuals = result.resid\n","\n","      # Fit a Gaussian distribution to the residuals\n","      mu, std = norm.fit(residuals)\n","\n","      # Compute the probability of a value being above or below the trend line\n","      # for each point in the time series\n","      z_scores = residuals / std\n","      df[\"prob_above_trend\"] = 1 - norm.cdf(z_scores)\n","      df[\"weighted-volume\"] = df[\"Close\"] * df[\"Volume\"]\n","      return df\n","\n","    def scale(self, timeseries):\n","      df = timeseries.reset_index()  # Reset the index of the DataFrame\n","      dates = df['Date']\n","      data_to_scale = df.drop(['Date', 'index'], axis=1)\n","      self.scaler = MinMaxScaler()\n","      scaled_data = self.scaler.fit_transform(data_to_scale)\n","      scaled_df = pd.concat([dates, pd.DataFrame(scaled_data, columns=data_to_scale.columns)], axis=1)\n","      return scaled_df.set_index('Date')  # Set the index back to 'Date'\n","\n","    def _reset_vars(self):\n","\n","        self._episode_ended = False        \n","        self.ledger = self.make_ledger_row()\n","        self.slippage = .01\n","        self.fee = .0025\n","        self.current_index = 0\n","        self.cash = 5000\n","        self.position_shares = 0\n","        self.cash_from_short = 0\n","        self.position_value = 0\n","        self.shares_owed = 0\n","        self.in_position = False\n","        self.in_long = False\n","        self.in_short = False\n","        self.last_action = -1\n","        self.long_profit = []\n","        self.short_profit = []\n","        self.long_entry = -1\n","        self.short_entry = -1\n","        self.rolling_score = 0\n","\n","        \n","\n","    def reset(self):\n","        # Reset the environment and return the initial time step\n","        self._reset_vars()\n","        return self._get_next_state(), {}\n","        #return self._get_next_state()\n","\n","    def reset_test(self):\n","        self._reset_vars()\n","        return self._get_next_state()\n","\n","    def step(self, action):\n","        \n","        action = int(action)\n","\n","        if len(self.expert_actions) > 0:\n","          info(\"_step:\", self.current_index, \" action: \", action, \" expert action is: \", self.expert_actions[self.current_index])\n","        else:\n","          info(\"_step:\", self.current_index, \" action: \", action)\n","\n","        self.last_action = action\n","        if self._episode_ended:\n","            # The last action ended the episode. Ignore the current action and start a new episode.\n","            return self.reset()\n","\n","        # Apply the action and update the environment state\n","        self._apply_action(action)\n","        \n","\n","        if self._is_episode_ended():\n","            reward = self._get_reward2()\n","            info(\"final reward:\", reward)\n","            return self._get_next_state(), reward, False, True, {}\n","            #return self._get_next_state(), reward, True, {}\n","        else:\n","            reward = self._get_reward2()\n","            info(\"current reward:\", reward)\n","            self.current_index += 1\n","            return self._get_next_state(), reward, False, False, {}\n","            #return self._get_next_state(), reward, False, {}\n","\n","    def _get_initial_state(self):\n","        # Return the initial state of the environment\n","        self.current_index = 0\n","        return self.env_block()\n","\n","    def should_stop(self):\n","      # if cash is negative\n","      if (self.total_value() <= 0):\n","        error(\"Bankrupt.\")\n","        return True\n","      return False\n","\n","\n","    def _apply_action(self, action):\n","        # Advance the environment by one time step and return the observation, reward, and done flag        \n","        verbose(\"step:\", \"index:\", self.current_index, \" of: \", self.final-1, \"action: \", int(action))\n","        self.update_position_value()\n","        if (self.current_index >= self.final - 1): #or  self.should_stop()\n","          error(\"********MARKING DONE\", \"index:\", self.current_index, \" of: \", self.final-1, \" cash: \", self.cash, \" value: \", self.position_value)\n","          if (self.position_shares != 0):\n","            verbose(\"done so closing position\")\n","            self.close_position()\n","          self._episode_ended = True\n","        else:\n","          self._episode_ended = False\n","\n","        # AI says hold\n","        if (action == 0):\n","          info(\"holding action = 0.\")\n","          self.closed_position = False\n","        # AI says long but we're already in a position\n","        elif (action == 1 and self.in_long):\n","          info(\"holding long.\")\n","          self.closed_position = False\n","        # AI says long, we're not in a position, so buy\n","        elif (action == 1 and not self.in_position):\n","          info(\"opening long.\")\n","          self.open_position()\n","          self.closed_position = False\n","        #AI says long, but we're short. Close the short, open a long.\n","        elif (action == 1 and self.in_short):\n","          info(\"closing short to open long.\")\n","          self.closed_position = True\n","          self.close_short() \n","          self.open_position()\n","        #AI says short, but we're already short  \n","        elif (action == 2 and self.in_short):\n","          info(\"holding short.\")\n","          self.closed_position = False\n","        #AI says short and we're not in a position so exit \n","        elif (action == 2 and not self.in_position):\n","          info(\"opening short.\")\n","          self.open_short()\n","          self.closed_position = False\n","        #AI says short but we're long, close it\n","        elif (action == 2 and self.in_long):\n","          info(\"closing long to open short\")\n","          self.close_position()\n","          info(\"opening short.\")\n","          self.open_short()\n","          self.closed_position = True\n","        else: #assume hold\n","          error(\"unknown state! holding:\", action, self.in_position, self.in_long, self.in_short)      \n","          self.closed_position = False\n","        self.update_position_value()\n","        \n","    \n","    def _get_next_state(self):\n","        # Calculate and return the next state based on the current state and action taken\n","        return self.env_block()\n","\n","    def get_price_with_slippage(self, price):\n","        return price + (price * self.slippage)\n","\n","\n","    def make_ledger_row(self):\n","        ledger = pd.DataFrame()\n","        ledger[\"Date\"] = []\n","        ledger[\"Side\"] = []\n","        ledger[\"Action\"]  = []\n","        ledger[\"Price\"] = []\n","        ledger[\"Profit\"] = []\n","        ledger[\"Fee\"] = []\n","        ledger[\"Value\"] = []\n","        \n","        return ledger\n","\n","    #todo you need to subtract fee from cash\n","    def open_position(self):\n","      self.in_position = True\n","      self.in_long = True\n","      df = self.orig_timeseries\n","      row = df.iloc[self.current_index,:]\n","      price = self.get_price_with_slippage(row[\"Close\"])\n","      self.position_shares = math.floor(self.cash/price)\n","      fee = self.cash * self.fee\n","      self.cash = 0    \n","      self.long_entry = price  \n","      ledger_row = self.make_ledger_row()\n","      ledger_row[\"Date\"] = [row[\"Date\"]]\n","      ledger_row[\"Side\"] = [\"long\"]\n","      ledger_row[\"Action\"] = [\"enter\"]\n","      ledger_row[\"Price\"] = [price]\n","      ledger_row[\"Fee\"] = [fee]\n","      ledger_row[\"Profit\"] = [0]\n","      ledger_row[\"Value\"] = [self.total_value()]\n","      \n","      self.ledger = pd.concat([self.ledger, ledger_row])\n","\n","    def open_short(self):\n","      self.in_position = True\n","      self.in_short = True\n","      df = self.orig_timeseries\n","      row = df.iloc[self.current_index,:]\n","      price = self.get_price_with_slippage(row[\"Close\"])\n","      max_short_pos = math.floor(self.cash/price)\n","      self.shares_owed = max_short_pos\n","      self.cash_from_short = (self.shares_owed * price)   \n","      fee = self.cash * self.fee \n","      self.cash = self.cash + self.cash_from_short\n","      self.short_entry = price\n","      verbose(\"Added cash on short: \", self.shares_owed * price, \" total: \", self.cash, \" took share debt:\", self.shares_owed)  \n","      ledger_row = self.make_ledger_row()\n","      ledger_row[\"Date\"] = [row[\"Date\"]]\n","      ledger_row[\"Side\"] = [\"short\"]\n","      ledger_row[\"Action\"] = [\"enter\"]\n","      ledger_row[\"Price\"] = [price]\n","      ledger_row[\"Fee\"] = [fee]\n","      ledger_row[\"Profit\"] = [0]\n","      ledger_row[\"Value\"] = [self.total_value()]\n","      self.ledger = pd.concat([self.ledger, ledger_row])\n","\n","    def close_position(self):\n","      self.in_position = False\n","      self.in_long = False\n","      df = self.orig_timeseries\n","      row = df.iloc[self.current_index,:]\n","      price = self.get_price_with_slippage(row[\"Close\"])\n","      value = price * self.position_shares\n","      fee = value * self.fee \n","      self.position_shares = 0\n","      self.cash = self.cash + value\n","      self.last_profit = (price - self.long_entry)/self.long_entry\n","      self.long_profit.append(self.last_profit)\n","      ledger_row = self.make_ledger_row()\n","      ledger_row[\"Date\"] = [row[\"Date\"]]\n","      ledger_row[\"Side\"] = [\"long\"]\n","      ledger_row[\"Action\"] = [\"exit\"]\n","      ledger_row[\"Price\"] = [price]\n","      ledger_row[\"Fee\"] = [fee]\n","      ledger_row[\"Profit\"] = [self.last_profit]\n","      ledger_row[\"Value\"] = [self.total_value()]\n","      self.ledger = pd.concat([self.ledger, ledger_row])\n","\n","    def close_short(self):\n","      self.in_position = False\n","      self.in_short = False\n","      df = self.orig_timeseries\n","      row = df.iloc[self.current_index,:]\n","      price = self.get_price_with_slippage(row[\"Close\"])\n","      value = price * self.shares_owed\n","      fee = value * self.fee\n","      self.shares_owed = 0\n","      self.cash_from_short = 0\n","      self.cash = self.cash - value\n","      self.last_profit = (self.short_entry - price)/self.short_entry\n","      self.short_profit.append(self.last_profit)\n","      ledger_row = self.make_ledger_row()\n","      ledger_row[\"Date\"] = [row[\"Date\"]]\n","      ledger_row[\"Side\"] = [\"short\"]\n","      ledger_row[\"Action\"] = [\"exit\"]\n","      ledger_row[\"Price\"] = [price]\n","      ledger_row[\"Fee\"] = [fee]\n","      ledger_row[\"Profit\"] = [self.short_entry - price]\n","      ledger_row[\"Value\"] = [self.total_value()]\n","      self.ledger = pd.concat([self.ledger, ledger_row])\n","\n","\n","    def _is_episode_ended(self):\n","        return self._episode_ended\n","        \n","    def update_position_value(self):\n","      df = self.orig_timeseries\n","      row = df.iloc[self.current_index,:]\n","      self.position_value = (row[\"Close\"] * self.position_shares) - (row[\"Close\"] * self.shares_owed)\n","      self.benchmark_value = row[\"Close\"] * self.benchmark_position_shares\n","\n","    def _get_reward2(self):\n","      '''\n","        What components should go into the reward?\n","        We can encode some of our desired behavior here, for example if we are at 99% chance of above trend we could reward long and vice versa\n","        We can remove cash from the reward calculation and simply treat it well we initially trade from. Any profits go into cash from trades, initial cash is penalized\n","        Consistency - making lots of profitable trades is better then big bets or not betting\n","      '''\n","      score = 0\n","      \n","      component1 = self.is_probable_set_up() \n","     \n","      return component1\n","\n","    def calculate_consistency_bonus(self):\n","      profitable_longs = list(filter(lambda x: x >= 0, self.long_profit))\n","      profitable_shorts = list(filter(lambda x: x >= 0, self.short_profit))\n","      profitable_trades = len(profitable_longs) + len(profitable_shorts)\n","      max = len(self.long_profit) + len(self.short_profit)\n","      if max == 0:\n","        return 0\n","      return profitable_trades/max * 100\n","\n","    def calculate_close_bonus(self):\n","      if self.closed_position and self.last_profit>0:\n","        return self.last_profit * 10\n","      else:\n","        return 0\n","\n","    def is_probable_set_up(self):\n","        df = self.orig_timeseries\n","        row = df.iloc[self.current_index, :]\n","        prob = row[\"prob_above_trend\"]\n","        action = self.last_action\n","\n","        print(\"action:\", action, \" prob:\", prob)\n","        if action == 1 and prob >= self.prob_high:\n","            return 100 # reward a highly probable long\n","        elif action == 2 and prob <= self.prob_low:\n","            return 100 # reward a highly probable short\n","        elif action == 0 and (self.prob_low > prob or prob < self.prob_high):\n","            return 0.01 # reward holding when probability is moderate\n","        else:\n","            #raise Exception(f\"WHY YOU WRONG? {self.current_index} {action} {prob}\")\n","            return -10 # penalize for taking actions against the criteria\n","\n","\n","    def _get_reward(self):\n","        df = self.orig_timeseries\n","        row = df.iloc[self.current_index,:]\n","        current_portfolio_value = self.total_value()\n","       \n","       \n","        #compare to a bench mark\n","        percentage_change = ((current_portfolio_value - self.benchmark_value) / self.benchmark_value) * 100\n","       \n","        verbose(\"reward states: \",\n","              \"\\nposition value: \", self.position_value,\n","              \"\\nlong shares: \", self.position_shares,\n","              \"\\nshort shares: \", self.shares_owed,\n","              \"\\nlong value:\", row[\"Close\"] * self.position_shares,\n","              \"\\nshort debt:\", row[\"Close\"] * self.shares_owed,\n","              \"\\ncash value:\", self.cash,\n","              \"\\ntoal value:\", current_portfolio_value,\n","              \"\\nbenchmark value:\", self.benchmark_value,\n","              \"\\nreal percentage_change: \", percentage_change\n","             )\n","        \n","        return percentage_change\n","        \n","\n","    def total_value(self):\n","      self.update_position_value()\n","      return self.position_value + self.cash\n","   \n","    def env_block(self):\n","      start_index = self.current_index\n","      end_index = self.current_index\n","      df = self.timeseries.reset_index().drop(['Date'], axis=1)\n","      block = df.iloc[start_index].to_numpy()\n","      return block\n"]},{"cell_type":"markdown","metadata":{"id":"U0WilZsVGcVh"},"source":["# Custom Policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_L58sc1GfG6"},"outputs":[],"source":["\n","class RNNStates(NamedTuple):\n","    pi: Tuple[th.Tensor, ...]\n","    vf: Tuple[th.Tensor, ...]\n","\n","class CustomActorCriticPolicy(MlpLstmPolicy):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.custom_actions = None\n","        self.custom_actions_index = 0\n","    def set_custom_actions(self, custom_actions):\n","        self.custom_actions = custom_actions\n","        self.custom_actions_index = 0\n","\n","    def forward(\n","        self,\n","        obs,\n","        lstm_states,\n","        episode_starts,\n","        deterministic = False):\n","        \"\"\"\n","        Forward pass in all the networks (actor and critic)\n","\n","        :param obs: Observation. Observation\n","        :param lstm_states: The last hidden and memory states for the LSTM.\n","        :param episode_starts: Whether the observations correspond to new episodes\n","            or not (we reset the lstm states in that case).\n","        :param deterministic: Whether to sample or use deterministic actions\n","        :return: action, value and log probability of the action\n","        \"\"\"\n","        # Preprocess the observation if needed\n","        features = self.extract_features(obs)\n","        if self.share_features_extractor:\n","            pi_features = vf_features = features  # alis\n","        else:\n","            pi_features, vf_features = features\n","        # latent_pi, latent_vf = self.mlp_extractor(features)\n","        latent_pi, lstm_states_pi = self._process_sequence(pi_features, lstm_states.pi, episode_starts, self.lstm_actor)\n","        if self.lstm_critic is not None:\n","            latent_vf, lstm_states_vf = self._process_sequence(vf_features, lstm_states.vf, episode_starts, self.lstm_critic)\n","        elif self.shared_lstm:\n","            # Re-use LSTM features but do not backpropagate\n","            latent_vf = latent_pi.detach()\n","            lstm_states_vf = (lstm_states_pi[0].detach(), lstm_states_pi[1].detach())\n","        else:\n","            # Critic only has a feedforward network\n","            latent_vf = self.critic(vf_features)\n","            lstm_states_vf = lstm_states_pi\n","\n","        latent_pi = self.mlp_extractor.forward_actor(latent_pi)\n","        latent_vf = self.mlp_extractor.forward_critic(latent_vf)\n","\n","        # Evaluate the values for the given observations\n","        values = self.value_net(latent_vf)\n","        distribution = self._get_action_dist_from_latent(latent_pi)\n","        if self.custom_actions is not None:\n","            # Retrieve the next custom action\n","            custom_action = self.custom_actions[self.custom_actions_index]\n","            self.custom_actions_index+=1\n","            if self.custom_actions_index >= len(self.custom_actions):\n","              self.custom_actions_index=0\n","            \n","            actions = np.array([custom_action])\n","            actions = torch.tensor(actions, dtype=torch.float32, device=self.device)\n","\n","            # Compute the log probabilities of the custom actions\n","            log_prob = distribution.log_prob(torch.tensor(actions, dtype=torch.float32, device=self.device))\n","        else:\n","            # If no custom actions are provided, use the actions generated by the actor network\n","            actions = distribution.get_actions(deterministic=deterministic)\n","            log_prob = distribution.log_prob(actions)\n","        return actions, values, log_prob, RNNStates(lstm_states_pi, lstm_states_vf)"]},{"cell_type":"markdown","metadata":{"id":"m_Gy1KZNErJL"},"source":["# Training Variants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXYM9DBAgjdZ"},"outputs":[],"source":["env_count = 0\n","def make_env_for(symbol, code, tail=-1, head=-1):\n","  tickerObj = yf.download(tickers = symbol, interval = \"1d\")\n","  df = pd.DataFrame(tickerObj)  \n","  if tail !=-1:\n","    df = df.tail(tail)\n","  if head !=-1:\n","    df = df.head(head)\n","  df = df.reset_index()  \n","  env = TraderEnv(symbol, df, code)\n","  return env\n","\n","def create_env(product_data, code=1):\n","    global env_count\n","    products = list(product_data.keys())\n","    product = products[env_count]\n","    env = TraderEnv(product, product_data[product], code)\n","    env_count+=1\n","    return env\n","\n","def timestr():  \n","  current_time = datetime.datetime.now()\n","  formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\") \n","  return formatted_time\n","\n","def full_train(curriculum_code, num_stocks, create=True):\n","    product_data = get_data_for_training(num_stocks)\n","    all_envs = [create_env(curriculum_code) for _ in range(len(product_data))]\n","    sorted_envs = sorted(all_envs, key=lambda env: env.product)\n","    if create:\n","      model = RecurrentPPO(\n","          \"MlpLstmPolicy\",\n","          sorted_envs[0],\n","          ent_coef=0.1,\n","          clip_range=0.3,\n","          verbose=1,\n","      )\n","    else:\n","      model = RecurrentPPO.load(os.path.join(model_path, \"baseline-recurrent-ppo\"))\n","    for env in sorted_envs:\n","      model.set_env(env)\n","      model.learn(total_timesteps=35000)\n","      model.save(os.path.join(model_path, \"baseline-recurrent-ppo\"))\n","\n","\n","def full_test(symbol, tail=1, head=-1):\n","    tickerObj = yf.download(tickers = symbol, interval = \"1d\")\n","    df = pd.DataFrame(tickerObj)\n","    if tail !=-1:\n","      df = df.tail(tail)\n","    if head !=-1:\n","      df = df.head(head)\n","\n","    df = df.reset_index()\n","    env = TraderEnv(symbol, df)\n","    model = RecurrentPPO.load(os.path.join(model_path, \"baseline-recurrent-ppo\"))\n","    obs, _ = env.reset()\n","    # cell and hidden state of the LSTM\n","    lstm_states = None\n","    num_envs = 1\n","    # Episode start signals are used to reset the lstm states\n","    episode_starts = np.ones((num_envs,), dtype=bool)\n","    done = False\n","    while not done:\n","      action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)\n","      obs, rewards, _, done, extra = env.step(action)\n","      episode_starts = done\n","\n","\n","def partial_test(env):\n","  model = RecurrentPPO.load(os.path.join(model_path, \"baseline-recurrent-ppo\"))\n","  model.set_env(env)\n","  model.policy.custom_actions = None\n","  obs = env.reset_test()\n","  done = False\n","  state = None\n","  while not done:\n","    action, state = model.predict(obs, state, episode_start=False, deterministic=False)\n","    # take the action and observe the next state and reward\n","    obs, reward, _, done, info_ = env.step(action)\n","    \n","def partial_train(env, steps=500, create=False):\n","  if create:\n","    model = RecurrentPPO(\n","        \"MlpLstmPolicy\",\n","        env,\n","        ent_coef=0.1,\n","        clip_range=0.3,\n","        verbose=1,\n","    )\n","  else:\n","    model = RecurrentPPO.load(os.path.join(model_path, \"baseline-recurrent-ppo\"))\n","    model.set_env(env)\n","    model.policy.custom_actions = None\n","  \n","  model.learn(total_timesteps=steps)\n","  model.save(os.path.join(model_path, \"baseline-recurrent-ppo\"))\n","\n","\n","def back_test_expert(symbol, days):\n","  env = make_env_for(symbol, 1, days)\n","  env.expert_opinion_df()\n","  obs, _ = env.reset()\n","  \n","  action = obs[-1]\n","  done = False\n","  while not done:\n","    print(\"Action:\", action)\n","    state, reward, _, done, _ = env.step(int(action))\n","    action = state[-1]\n","    print(\"Reward:\", reward, \" for action: \", action, \"on probability: \", state[3])\n","  return env\n","\n","def guided_training(symbol, create, steps=250000, tail=365, head=-1):\n","  env = make_env_for(symbol, 1, tail, head)\n","  if create:\n","    ppo_agent = RecurrentPPO(\n","        CustomActorCriticPolicy,\n","        env,\n","        ent_coef=0.1,\n","        clip_range=0.3,\n","        verbose=1,\n","    )\n","  else:\n","    ppo_agent = RecurrentPPO.load(os.path.join(model_path, \"baseline-recurrent-ppo\"))\n","    ppo_agent.set_env(env)\n","\n","  state_action_data = env.expert_opinion() \n","  custom_actions = [action for _, action in state_action_data]\n","  ppo_agent.policy.custom_actions = custom_actions\n","  ppo_agent.learn(total_timesteps=steps)\n","  ppo_agent.save(os.path.join(model_path, \"baseline-recurrent-ppo\"))\n","  return env\n","\n","def full_guided_train(num_stocks, create=True):\n","    product_data = get_data_for_training(num_stocks)\n","    all_envs = [create_env(product_data) for _ in range(len(product_data))]\n","    sorted_envs = sorted(all_envs, key=lambda env: env.product)\n","    if create:\n","      ppo_agent = RecurrentPPO(\n","          \"MlpLstmPolicy\",\n","          sorted_envs[0],\n","          ent_coef=0.1,\n","          clip_range=0.3,\n","          verbose=1,\n","      )\n","    else:\n","      ppo_agent = RecurrentPPO.load(os.path.join(model_path, \"baseline-recurrent-ppo\"))\n","    for env in sorted_envs:\n","      ppo_agent.set_env(env)\n","      state_action_data = env.expert_opinion() \n","      custom_actions = [action for _, action in state_action_data]\n","      ppo_agent.policy.custom_actions = custom_actions\n","      ppo_agent.learn(total_timesteps=35000)\n","      ppo_agent.save(os.path.join(model_path, \"baseline-recurrent-ppo\"))\n"]},{"cell_type":"markdown","metadata":{"id":"l3rGYOFNFZ4u"},"source":["# Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LaJ97E3EFJ_Z","outputId":"bf995d42-78b5-4dd5-9849-6ab73aef7e09"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r[*********************100%***********************]  1 of 1 completed\n"]}],"source":["env1 = guided_training(\"SPY\", False, 250000, 2500)\n","\n","env2 = make_env_for(\"SPY\", 1, 2500)\n","partial_train(env2, 250000, False)\n","\n","env3 = make_env_for(\"SPY\", 1, 2500)\n","partial_test(env3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"McA2aDwvlj2n"},"outputs":[],"source":["env3.ledger"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["CF9oPeaEEh2x","U0WilZsVGcVh","m_Gy1KZNErJL"],"machine_shape":"hm","provenance":[{"file_id":"1r-xHJcGQ0wmO6LiJNghVtLnMFjQvGggP","timestamp":1680305133819}],"authorship_tag":"ABX9TyOTsE+mluW4fAxbDeLgvyli"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}