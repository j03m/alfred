{"cells":[{"cell_type":"markdown","source":["# Todo:\n","\n","* Remodel the agent to ~PPO~ (looks like PPO isn't implemented in keras-rl) DDQN leveraging its own LSTM, test against DQN. Might be here tho: https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/rl/ipynb/ppo_cartpole.ipynb\n"," \n","* If that isn't successful, we can try including those values with the LSTM predictions in the data that will go the Agent\n","* Can an SDAE feature extraction make for a better lstm prediction?\n","* Consider that our applicaton of Attention in the LSTMs is wrong :( fix it.\n","* Find an article discussing LSTMs as feature extractors\n","* Good read: https://arxiv.org/pdf/2212.02721.pdf\n","\n","More sources:\n","\n","* LSTM with CONV1 https://shivapriya-katta.medium.com/time-series-forecasting-using-conv1d-lstm-multiple-timesteps-into-future-acc684dcaaa\n","\n","* LSTMs and Attention: https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b\n","\n","\n"],"metadata":{"id":"R65UIvwy98NM"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17737,"status":"ok","timestamp":1677443443840,"user":{"displayName":"Joseph Mordetsky","userId":"12205681246264358771"},"user_tz":300},"id":"l9kHEPrOjtDT","outputId":"4d469849-7454-49d0-caef-5167a138d994"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","\n","sys.path.insert(0,'/content/drive/My Drive/ml-trde-notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAor5rKVjwH4"},"outputs":[],"source":["# ***** WARNING : Install deps - This will BUILD TALib and takes a while!\n","%run -i '/content/drive/My Drive/ml-trde-notebooks/installs.ipynb'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_TPNwuxVqjg"},"outputs":[],"source":["!pip install keras-rl2"]},{"cell_type":"code","source":["!pip install -Uqq ipdb\n","import ipdb\n"],"metadata":{"id":"-MfAptcUEAr0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pdb off"],"metadata":{"id":"qL1LKew_bK3o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"qM1_qjitg9Wi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOX5W1ELjwuD"},"outputs":[],"source":["# Executes this notebook in our space, making all of its functions/globals availablev\n","_models = None\n","replace = False\n","\n","if 'models' in globals():\n","  _models = models\n","  should_load = models_loaded\n","  replace = True\n","%run -i '/content/drive/My Drive/ml-trde-notebooks/common.ipynb'\n","\n","if replace:\n","  models_loaded = should_load\n","  models = _models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcnmpe_JjyuZ"},"outputs":[],"source":["# Executes this notebook in our space, making all of its functions/globals available\n","%run -i '/content/drive/My Drive/ml-trde-notebooks/backtest-common.ipynb'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYGizSJ2l49A"},"outputs":[],"source":["def download_stocks():\n","  # We don't want to train against crypto\n","  return get_all_stock_timerseries_for_csv(\"training_tickers3.csv\", 3500,-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhpuepDhV_5s"},"outputs":[],"source":["LENGTH_OF_STOCK_TRAINGING_DATA = 145 #I might need to fix this, but the model is tied to the number of symbols we trained on\n","def download_crypto():\n","  return get_all_product_timeseries(-1, 180, LENGTH_OF_STOCK_TRAINGING_DATA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDa4DZBTTHEh"},"outputs":[],"source":["def save_dict(product_data, crypto=False):\n","  # Write each dataframe to disk\n","  for key, df in product_data.items():\n","    df.to_csv(data_path + \"/dqn-preds-\" + key + \".csv\", index=False)\n","\n","  # Convert the keys of the dictionary to a dataframe\n","  keys_df = pd.DataFrame({\"keys\": list(product_data.keys())})\n","\n","  # Save the keys dataframe to disk as a CSV file\n","  if crypto:\n","    keys_df.to_csv(data_path + \"/dqn-preds-keys-crypto.csv\", index=False)\n","  else:\n","    keys_df.to_csv(data_path + \"/dqn-preds-keys.csv\", index=False)\n","\n","def load_dict(crypto=False):\n","  if crypto:\n","    keys = pd.read_csv(data_path + \"/dqn-preds-keys-crypto.csv\")\n","  else:\n","    keys = pd.read_csv(data_path + \"/dqn-preds-keys.csv\")\n","  product_data = {}\n","  # Iterate over the keys\n","  for index, row in keys.iterrows():\n","    key = row[\"keys\"]\n","    product_data[key] = pd.read_csv(data_path + \"/dqn-preds-\" + key + \".csv\")\n","  return product_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldveaUJQNN2B"},"outputs":[],"source":["def transform_to_predictions(product_data):\n","  product_data_= append_predictions(product_data, True)\n","  return product_data_"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":35517,"status":"ok","timestamp":1677342549510,"user":{"displayName":"Joseph Mordetsky","userId":"12205681246264358771"},"user_tz":300},"id":"zHko2Bisl6Dj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"103fcddd-fe30-4111-fc29-9e5bbb57523e"},"outputs":[{"output_type":"stream","name":"stdout","text":["symbols: 145\n","min length: 180\n"]}],"source":["# we need to generate a massive action set actions is an array of ints, so we need some sort of mapping.\n","actions = []\n","action_map = []\n","stock_actions = [0,1,-1] #hold, buy, short\n","product_map = {}\n","index = 0\n","product_id = 0\n","\n","from_disk = True\n","use_crypto = True\n","\n","# this is dumb, but since I trained and saved the model with 145 different \n","# stocks to select from, I now have to use only that many products\n","MAGIC_SHAPE_NUMBER = 145\n","\n","if from_disk:\n","  product_data = load_dict(use_crypto)\n","  first_key = list(product_data.keys())[0]\n","  length = len(product_data[first_key])\n","elif use_crypto:\n","  print(\"load models? \", models_loaded)\n","  if not models_loaded:\n","    load_all_models()\n","  print(\"models loaded? \", models_loaded)\n","  product_data = download_crypto()\n","  product_data = transform_to_predictions(product_data)\n","  save_dict(product_data, True)\n","else:\n","  if not models_loaded:\n","    load_all_models()\n","  product_data = download_stocks()\n","  product_data = transform_to_predictions(product_data)\n","  save_dict(product_data)\n","\n","min_len = min(len(df) for df in product_data.values())\n","print(\"symbols:\", len(product_data.keys()))\n","print(\"min length:\", min_len)\n","for name, df in product_data.items():\n","    product_data[name] = df.head(min_len)\n","\n"]},{"cell_type":"code","source":["print(len(product_data.keys()), \" \", MAGIC_SHAPE_NUMBER)\n","assert(len(product_data.keys()) == MAGIC_SHAPE_NUMBER)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u97I1SORrrTn","executionInfo":{"status":"ok","timestamp":1677342549511,"user_tz":300,"elapsed":24,"user":{"displayName":"Joseph Mordetsky","userId":"12205681246264358771"}},"outputId":"38a62f1d-0787-4b4a-9323-2b791088123e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["145   145\n"]}]},{"cell_type":"markdown","source":["# DQN Agent\n"],"metadata":{"id":"slqNKo8zlVlv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_c_8geubXdOl"},"outputs":[],"source":["for product, data in product_data.items():\n","  product_map[product] = product_id\n","  product_id+=1\n","  for action in stock_actions:\n","    actions.append(index) #action for the AI is number: index\n","    action_map.append([product, action]) #our lookup for the env will be product + buy, sell, hold\n","    index+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUrY916Y2bMB"},"outputs":[],"source":["from rl.agents.dqn import DQNAgent\n","from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n","from rl.memory import SequentialMemory\n","from rl.core import Processor\n","from rl.callbacks import FileLogger, ModelIntervalCheckpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_b8VAix5xDgg"},"outputs":[],"source":["class BackTestingEnv():\n","    def __init__(self, product_data):\n","        # Initialize the environment and retrieve stock data from a data source\n","        self.product_data = product_data\n","        self.current_index = 0\n","        self.cash = 5000\n","        self.position_shares = 0\n","        self.position_value = 0\n","        first_key = list(product_data.keys())[0]\n","        self.final = len(product_data[first_key])\n","        self.current_product = None\n","        self.ledger = self.make_ledger_row()\n","        self.slippage = .01\n","        self.fee = .0025\n","\n","    def make_ledger_row(self):\n","      ledger = pd.DataFrame()\n","      ledger[\"Date\"] = []\n","      ledger[\"Product\"] = []\n","      ledger[\"Side\"] = []\n","      ledger[\"Action\"]  = []\n","      ledger[\"Price\"] = []\n","      ledger[\"Fee\"] = []\n","      return ledger\n","\n","    def reset(self):\n","        # Reset the environment and return the first observation\n","        info(\"Reset!!!!\")\n","        self.current_index = 0\n","        self.cash = 5000\n","        self.position_shares = 0\n","        self.position_value = 0\n","        self.current_product = None\n","        self.ledger = self.make_ledger_row()\n","        return self.env_block()\n","\n","    def step(self, action):\n","        # Advance the environment by one time step and return the observation, reward, and done flag\n","        self.current_index += 1\n","        info(\"step:\", \"index:\", self.current_index, \" of: \", self.final-1, \" cash: \", self.cash, \" value: \", self.position_value)\n","\n","        if (self.current_index >= self.final - 1 or self.calc_reward() <= 0):\n","          info(\"********MARKING DONE\", \"index:\", self.current_index, \" of: \", self.final-1, \" cash: \", self.cash, \" value: \", self.position_value)\n","          if (self.current_product != None):\n","            info(\"done so closing: \", self.current_product)\n","            self.close_position()\n","          reward = self.calc_reward()\n","          return self.env_block(), reward, True, {}\n","        else:\n","          done = False\n","\n","        # convert action to product and action\n","        [product, stock_action] = action_map[action]\n","\n","        if ((product == self.current_product and stock_action == 1) or stock_action == 0):\n","          info(\"holding: \", self.current_product)\n","          self.update_position_value()\n","          reward = self.calc_reward()\n","          return self.env_block(), reward, done, {}\n","        elif (product == self.current_product and stock_action == -1):\n","          info(\"closing: \", self.current_product)\n","          self.close_position()\n","          reward = self.calc_reward()\n","          return self.env_block(), reward, done, {}\n","        elif (product != self.current_product and self.current_product != None and stock_action == 1):\n","          info(\"closing and opening: \", self.current_product, product)\n","          self.close_position()\n","          self.open_position(product)\n","          reward = self.calc_reward()\n","          return self.env_block(), reward, done, {}\n","        elif (self.current_product == None and stock_action == 1):\n","          info(\"opening: \", self.current_product, product)\n","          self.open_position(product)\n","          reward = self.calc_reward()\n","          return self.env_block(), reward, done, {}\n","        else:\n","          info(\"invalid\")\n","          reward = self.calc_reward()\n","          return self.env_block(), reward, done, {}\n","\n","    def calc_reward(self):\n","      info(\"current reward: \", self.position_value + self.cash)\n","      return self.position_value + self.cash\n","\n","    def get_price_with_slippage(self, price):\n","        return price + (price * self.slippage)\n","\n","    def open_position(self, product):\n","      info(\"opening position: \", product)\n","      df = self.product_data[product]\n","      row = df.iloc[self.current_index,:]\n","      price = self.get_price_with_slippage(row[\"Close\"])\n","      self.position_shares = math.floor(self.cash/price)\n","      self.position_value  = self.position_shares * price\n","      self.cash = 0\n","      self.current_product = product\n","      ledger_row = self.make_ledger_row()\n","      ledger_row[\"Product\"] = [product]\n","      ledger_row[\"Side\"] = [1]\n","      ledger_row[\"Action\"] = [\"enter\"]\n","      ledger_row[\"Price\"] = [price]\n","      ledger_row[\"Fee\"] = [self.fee]\n","      self.ledger = self.ledger.append(ledger_row)\n","\n","\n","    def close_position(self):\n","      info(\"closing position: \", product)\n","      df = self.product_data[self.current_product]\n","      row = df.iloc[self.current_index,:]\n","      price = self.get_price_with_slippage(row[\"Close\"])\n","      value = price * self.position_shares\n","      self.position_shares = 0\n","      self.position_value = 0\n","      self.cash = value\n","      ledger_row = self.make_ledger_row()\n","      ledger_row[\"Product\"] = [self.current_product]\n","      ledger_row[\"Side\"] = [1]\n","      ledger_row[\"Action\"] = [\"exit\"]\n","      ledger_row[\"Price\"] = [price]\n","      ledger_row[\"Fee\"] = [self.cash * self.fee]\n","      self.ledger = self.ledger.append(ledger_row)\n","      self.current_product = None\n","\n","    def update_position_value(self):\n","      if self.current_product == None:\n","        return\n","      df = self.product_data[self.current_product]\n","      row = df.iloc[self.current_index,:]\n","      self.position_value = row[\"Close\"] * self.position_shares\n","    \n","\n","    def render(self, mode='human'):\n","        # Render the environment for human consumption\n","        pass\n","\n","    def env_block(self):\n","      selected_rows_list = []\n","      for product, df in self.product_data.items():\n","        row = df.iloc[self.current_index,:]\n","        row[\"product\"] = product_map[product]\n","        row = row.drop(\"Close\")\n","        selected_rows_list.append(row.values)\n","      np_selected_rows = np.array(selected_rows_list)\n","      return np_selected_rows\n","\n","env = BackTestingEnv(product_data)\n","first_key = list(product_data.keys())[0]\n","# NOTE: We drop Close (in favor of scaled_close) but add product so this is a wash\n","cols = len(product_data[first_key].columns)\n","symbols = len(env.product_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-whH-6SIKJAn","executionInfo":{"status":"error","timestamp":1677351477981,"user_tz":300,"elapsed":10,"user":{"displayName":"Joseph Mordetsky","userId":"12205681246264358771"}},"colab":{"base_uri":"https://localhost:8080/","height":236},"outputId":"9651f022-6f03-4e53-a305-6fce46f1e081"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-bde2b4655af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnb_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Number of possible actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ruh oh: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mweights_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/dqn_trader_weights.h5f'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'actions' is not defined"]}],"source":["import math\n","#np.random.seed(123)\n","\n","\n","# study: https://github.com/tensorneko/keras-rl2/blob/master/examples/dqn_cartpole.py\n","\n","\n","\n","nb_actions = len(actions) # Number of possible actions\n","print(\"ruh oh: \", symbols)\n","weights_filename = model_path + '/dqn_trader_weights.h5f'\n","midpoint_filename = model_path + \"/dqn_trader_weights_250000.h5f\"\n","\n","    \n","# Next, we build our model. We use the same model that was described by Mnih et al. (2015).\n","model = keras.models.Sequential()\n","model.add(keras.layers.Flatten(input_shape=(1, symbols,cols)))\n","model.add(keras.layers.Dense(16))\n","model.add(keras.layers.Activation('relu'))\n","model.add(keras.layers.Dense(16))\n","model.add(keras.layers.Activation('relu'))\n","model.add(keras.layers.Dense(16))\n","model.add(keras.layers.Activation('relu'))\n","model.add(keras.layers.Dense(nb_actions))\n","model.add(keras.layers.Activation('linear'))\n","print(model.summary())\n","\n","# Finally, we configure and compile our agent. You can use every built-in tensorflow.keras optimizer and\n","# even the metrics!\n","memory = SequentialMemory(limit=1000000, window_length=1)\n","\n","\n","# Select a policy. We use eps-greedy action selection, which means that a random action is selected\n","# with probability eps. We anneal eps from 1.0 to 0.1 over the course of 1M steps. This is done so that\n","# the agent initially explores the environment (high eps) and then gradually sticks to what it knows\n","# (low eps). We also set a dedicated eps value that is used during testing. Note that we set it to 0.05\n","# so that the agent still performs some random actions. This ensures that the agent cannot get stuck.\n","#policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n","#                              nb_steps=1000000)\n","policy = BoltzmannQPolicy()\n","\n","# The trade-off between exploration and exploitation is difficult and an on-going research topic.\n","# If you want, you can experiment with the parameters or use a different policy. Another popular one\n","# is Boltzmann-style exploration:\n","# policy = BoltzmannQPolicy(tau=1.)\n","# Feel free to give it a try!\n","\n","dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n","              target_model_update=1e-2, policy=policy)\n","\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n","checkpoint_weights_filename = model_path + '/dqn_trader_weights_{step}.h5f'\n","log_filename = data_path + '/dqn_{args.env_name}_log.json'\n","callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n","callbacks += [FileLogger(log_filename, interval=100)]\n","train = False\n","if train:\n","  if os.path.exists(midpoint_filename):\n","    dqn.model.load_weights(midpoint_filename)\n","  \n","  dqn.fit(env, callbacks=callbacks, nb_steps=1750000, visualize=False,  log_interval=10000, verbose=2)\n","\n","  # After training is done, we save the final weights one more time.\n","  dqn.save_weights(weights_filename, overwrite=True)\n","\n","  \n","else:\n","    dqn.load_weights(weights_filename)\n","    dqn.test(env, nb_episodes=10, visualize=False)"]},{"cell_type":"code","source":["env.calc_reward()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S3IxRuESs_8Z","executionInfo":{"status":"ok","timestamp":1677343315708,"user_tz":300,"elapsed":124,"user":{"displayName":"Joseph Mordetsky","userId":"12205681246264358771"}},"outputId":"706a040a-2955-4678-98db-57b48d0aea1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["current reward:  5336.043211\n"]},{"output_type":"execute_result","data":{"text/plain":["5336.043211"]},"metadata":{},"execution_count":20}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMVIXEZNuB1duaRzTLLeP6R"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}